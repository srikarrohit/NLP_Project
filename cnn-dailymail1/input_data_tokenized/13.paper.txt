

A system can defend itself against adversarial machine learners by randomly switching the feature mask it is using , by detecting unusual distributions of system inputs and reacting defensively to them , or by using other methods . Authorship attribution can be discovered using adversarial machine learning to learn the patterns between the character unigrams and authors . An adversarial machine learner can learn the mappings between system inputs and outputs and learn the Neural Network -LRB- NN -RRB- or Support Vector Machine -LRB- SVM -RRB- .

A defensive system can detect the system inputs from an adversarial machine learner by comparing the normal probability distribution of the inputs versus the current probability distribution
% and conditional probabilities
% of the feature masks
of
% and
the inputs and detecting a difference between them . If a difference is detected between the normal inputs to the system and the current inputs to the system based on the probability , then it can be assumed that the current inputs may come from an adversarial machine learner which
has learned
% is attempting to learn
the input-output mapping to model the NN .
Also , if it is discovered that the inputs are along a certain decision boundary that the system has , then the defending system can become wary of the inputs and suspect them of being from an adversarial machine learning attacker system .
Since the adversarial machine learner can develop a NN or SVM to model the system , given the input-output mappings , the adversarial machine learning system can then use knowledge of the decision boundaries in the model to inform creating inputs that will `` trick '' the system that the adversarial machine learning system is attacking .

Generative Adversarial Networks -LRB- GANs -RRB- are a type of adversarial artificial intelligence . The discriminator operates based on a neural network that has decision boundaries and makes decisions based on inputs . The generative network , the adversary , learns the distribution in order to `` fake out '' the discriminator . Generative Adversarial Networks learn the distribution , create synthetic data , and can then trick the discriminator system and its decision boundaries . Examples of GANs tricking systems are `` deep fakes '' , where an attacker creates an input to trick the decision boundaries of the original system . The analysis of variance , such as an F-test , ANOVA test , or Student t-Test , can be done . Then , the P-value and T-values can be analyzed to make sure that the guessed value is correct .

The seven phases in the Machine Learning Model Kill Chain are the :
\ begin -LCB- enumerate -RCB-
\ item Reconnaissance -LRB- Recon -RRB- Phase
\ item Weaponization Phase
\ item Delivery Phase
\ item Exploitation
\ item Installation
\ item Command and Control
\ item Action
\ end -LCB- enumerate -RCB-

% The three phases in the Machine Learning Model Kill Chain are the Reconnaissance -LRB- Recon -RRB- Phase , the Weaponization Phase , and the Delivery Phase .
During the Reconnaissance -LRB- Recon -RRB- Phase , the Machine Learning -LRB- ML -RRB- models are determined . The ML models are used to protect the defending system from the type of attacks which are to be launched by the attacking system . Then , during the Weaponization Phase , the results of probes are used in an effort to develop an attack on the defending system by learning the defending system 's decision boundary . During the Decision Phase , the defending system 's decision boundary is attacked by the attacking adversarial machine learning system . During the Exploitation Phase , the adversarial machine learning system gathers deeper information about the defending system 's underlying model . The attacking system learns how the defending system 's model will be tuned , how fast new rules can be formed , and how threats are ranked . During the Installation Phase , new rules , or features , that will allow future attacks to happen , are set up . During the Command and Control Phase , a hidden command and control channel is set up to allow for expansion of the attack . Finally , during the Action Phase , the attackers act on their main objective -LCB- Nyugen 2017 -RCB- .

\ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks are developed , and the system switches between the \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks to get roughly the same results . If an attacker knows the coding language that the system is written in -LRB- Python , in this case -RRB- , the random generator , and the seed , then the attacker can figure out the \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks .

If the number of features is reduced , the accuracy will increase . Feature reduction is done \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- times to develop the \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks . The cycle of attack by the Adversarial Author -LRB- Attacking System -RRB- and defense by the Authorship Attribution System -LRB- AAS -RRB- -LRB- Defending System -RRB- is shown in Figure ~ \ ref -LCB- figure _ AdvAuthor _ AAS -RCB- .

\ begin -LCB- figure -RCB- % -LSB- H -RSB-
\ begin -LCB- center -RCB-
\ setlength -LCB- \ unitlength -RCB- -LCB- 0.012500 in -RCB-
\ includegraphics -LSB- width = 40 mm , scale = 1.5 -RSB- -LCB- AdvAuthor _ AAS.png -RCB-
\ end -LCB- center -RCB-
\ caption -LCB- Cycle of Adversarial Author -LRB- Attacking System -RRB- vs. Authorship Attribution System -LRB- Defending System -RRB- -RCB-
\ label -LCB- figure _ AdvAuthor _ AAS -RCB-
\ end -LCB- figure -RCB-

Acceptable ranges of drops in accuracy by the AAS whilst under attack are less than \ begin -LCB- math -RCB- 11 \ end -LCB- math -RCB- percent . The baseline accuracy of the AAS is between \ begin -LCB- math -RCB- 60 \ end -LCB- math -RCB- to \ begin -LCB- math -RCB- 80 \ end -LCB- math -RCB- percent , depending on which algorithm is being used . Therefore , an \ begin -LCB- math -RCB- 11 \ end -LCB- math -RCB- percent drop in accuracy on \ begin -LCB- math -RCB- 60 \ end -LCB- math -RCB- to \ begin -LCB- math -RCB- 80 \ end -LCB- math -RCB- percent accuracy would result in \ begin -LCB- math -RCB- 49 \ end -LCB- math -RCB- to \ begin -LCB- math -RCB- 69 \ end -LCB- math -RCB- percent accuracy . % would result in \ begin -LCB- math -RCB- 53.4 \ end -LCB- math -RCB- to \ begin -LCB- math -RCB- 71.2 \ end -LCB- math -RCB- percent accuracy . % would result in \ begin -LCB- math -RCB- 49 \ end -LCB- math -RCB- to \ begin -LCB- math -RCB- 69 \ end -LCB- math -RCB- percent accuracy . %


%
%
% % % type of Authorship attribution uses machine learning to recognize script samples as adversarial or friendly . % Artificial Intelligence and Machine Learning Security are applications for adversarial machine learning algorithms , while Artificial Intelligence in Cybersecurity is another application .
% % Generative Adversarial Networks -LRB- GANs -RRB- are a type of adversarial artificial intelligence . The discriminator operates based on a neural network that has decision boundaries and makes decisions based on inputs . The generative network , the adversary , learns the distribution in order to `` fake out '' the discriminator . Generative Adversarial Networks learn the distribution , create synthetic data , and can then trick the discriminator system and its decision boundaries . Examples of GANs tricking systems are `` deep fakes '' , where an attacker creates an input to trick the decision boundaries of the original system . The analysis of variance , such as an F-test , ANOVA test , or Student t-Test , can be done . Then , the P-value and T-values can be analyzed to make sure that the guessed value is correct .
%
% % There are three phases in the Machine Learning Model Kill Chain : Recon Phase , Weaponization , and Delivery . The Recon Phase consists of determining which ML models to use to protect the type of attacks which are desired to be launched . % The information that is known about the overall system is ...
% % % The models are -LRB- not -RRB- open-source . % The kinds of Social Engineering available are ...
% % The Weaponization Phase consists of using the results of probes in an effort to develop an attack on the system by learning the system 's decision boundary . The Decision Phase consists of attacking the decision boundary .
% % \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks are developed , and the system switches between the \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks to get roughly the same results . If an attacker knows the coding language that the system is written in -LRB- Python , in this case -RRB- , the . random generator , and the seed , then the attacker can figure out the \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks .
%
Term Frequency-Inverse Document Frequency -LRB- TFIDF -RRB- is a numerical statistic that is intended to reflect how important a letter is in a character unigram . TFIDF is used as a weighting factor in searches , and the value increases proportionally to the number of times a letter appears in the character unigram and is offset by the number of character unigrams in the set that contain the letter .

When the simple -LRB- unweighted -RRB- Euclidean distance is used , normalization provides an equal weighting to all features , whereas some features would have more importance than others when normalization is not used . Normalization of the training data is done by subtracting the mean and dividing the mean and standard deviation of the training data .

Standardization provides a way to scale the test data with the mean and standard deviation of the training data .

It is important for a defensive system to establish a sense of normalcy for inputs . A measure of normalcy is created by the algorithm knowing the distribution of normal inputs . If the distribution of the inputs has changed , a notification is made by the defensive algorithm to alert the system that there is something that has changed and the machine learning algorithm is receiving abnormal inputs . This violates the stationarity assumption , since the inputs are not normal and expected inputs . The inputs with abnormal distributions can trick the decision boundary and create bad results from the machine learning algorithm .

In machine learning , Naïve Bayes classifiers are a family of simple `` probabilistic classifiers '' based on applying Bayes ' theorem with strong -LRB- naïve -RRB- independence assumptions between the features . They are among the simplest Bayesian network models .

Naïve Bayes classifiers are highly scalable , requiring a number of parameters linear in the number of variables -LRB- features/predictors -RRB- in a learning problem . Maximum-Likelihood training can be done by evaluating a closed-form expression , which takes linear time , rather than by expensive iterative approximation as used for many other types of classifiers .

In the statistics and computer science literature , Naïve Bayes models are known under a variety of names , including simple Bayes and independent Bayes . All these names reference the use of Bayes ' theorem in the classifier 's decision rule , but Naïve Bayes is not -LRB- necessarily -RRB- a Bayesian method .

The discussion so far has derived the independent feature model , that is , the Naïve Bayes probability model . The Naïve Bayes classifier combines this model with a decision rule . One common rule is to pick the hypothesis that is most probable ; this is known as the Maximum A-Posteriori -LRB- MAP -RRB- decision rule .

When dealing with continuous data , a typical assumption is that the continuous values associated with each class are distributed according to a normal -LRB- or Gaussian -RRB- distribution . For example , suppose the training data contains a continuous attribute , \ begin -LCB- math -RCB- x \ end -LCB- math -RCB- . The data is first segmented by the class , and then compute the mean and variance of \ begin -LCB- math -RCB- x \ end -LCB- math -RCB- in each class .

The algorithms are sorted into equivalence classes based on whether there are statistically significant differences between the algorithms . The ANOVA and Student t-Tests are used to determine statistical significance and to compare the algorithms .


% % There are \ begin -LCB- math -RCB- \ end -LCB- math -RCB- equivalence classes . The LSVM with Normalization , Standardization , and TFIDF and the RBFNN with Normalization are in the same equivalence class , while the FFNN with Standardization and the RBFNN with Standardization and TFIDF are in a different equivalence class .
%
%
% % GEFeS is done for the RBFSVM , LSVM , and FFNN . The multi-objective optimization problem is not

@label

An Adversarial System to attack and an Authorship Attribution System -LRB- AAS -RRB- to defend itself against the attacks are analyzed
@label

Defending a system against attacks from an adversarial machine learner can be done by randomly switching between models for the system , by detecting and reacting to changes in the distribution of normal inputs , or by using other methods
@label
Adversarial machine learning is used to identify a system that is being used to map system inputs to outputs
@label

% A GEFeS is developed to evolve \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- feature masks for the three baseline machine learning techniques
@label
A unigram feature extractor is developed
@label

% % The improvement of using feature selection over the baselines is
@label
% explained
@label

% Over the \ begin -LCB- math -RCB- 30 \ end -LCB- math -RCB- runs for each GEFeS , the average and standard deviation of the features being presented -LRB- this is known as the feature consistency -RRB- is
@label

% The most consistent features are
@label

Three types of machine learners are using for the model that is being attacked
@label
The machine learners that are used to model the system being attacked are a Radial Basis Function Support Vector Machine , a Linear Support Vector Machine , and a Feedforward Neural Network
@label
The feature masks are evolved using accuracy as the fitness measure
@label
The system defends itself against adversarial machine learning attacks by identifying inputs that do not match the probability distribution of normal inputs
@label
The system also defends itself against adversarial attacks by randomly switching between the feature masks being used to map system inputs to outputs
@label



